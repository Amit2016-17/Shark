
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Principal Component Analysis &#8212; Shark 3.0a documentation</title>
    <script type="text/x-mathjax-config">
        MathJax.Ajax.timeout = 5000;  <!-- 5 second rather than 15 seconds timeout for file access -->
        <!-- ( nicked from https://groups.google.com/forum/#!msg/mathjax-users/HKA2lNqv-OQ/_Yv72L7MtjYJ ) -->
    </script>
    <link rel="stylesheet" href="../../../_static/mt_sphinx_deriv.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML&delayStartupUntilConfig"></script>
    <link rel="shortcut icon" href="../../../_static/shark16.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Linear Discriminant Analysis" href="lda.html" />
    <link rel="prev" title="Serialization" href="../concepts/misc/serialization.html" />
    <link rel="stylesheet" href="../../../_static/mt_sphinx_shark.css" type="text/css" />
    <script type="text/javascript">
       window.MathJax || document.write('<script src="../../../index/../../../../../contrib/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML&delayStartupUntilConfig"><\/script>');
       <!--delay nicked from https://groups.google.com/forum/#!msg/mathjax-users/J-36V22-G9Q/AW3ncCbJzS8J -->
    </script>
    <script src="../../../index/../../../../mlstyle.js"></script>
    <!-- nicked from https://groups.google.com/forum/#!msg/mathjax-users/J-36V22-G9Q/AW3ncCbJzS8J -->

  </head><body>

    <div id="shark_old">
        <div id="wrap">
            <div id="header">
                <div id="site-name"><a href="/shark/index.html">Shark machine learning library</a></div>
                <ul id="nav">
                    <li  class="first" >
                        <a href="../../installation.html">Installation</a>
                    </li>
                    <li  class="active" >
                        <a href="../tutorials.html">Tutorials</a>
                    </li>
		    <li  class="first" >
                        <a href="../../benchmark.html">Benchmarks</a>
                    </li>
		    <li  class="first" >
                        <a href="../../../index/../../../../doxygen_pages/html/classes.html">Documentation</a>
                        <ul>
                            <li><a href="../../quickref/quickref.html">Quick references</a></li>
                            <li><a href="../../../index/../../../../doxygen_pages/html/classes.html">Class list</a></li>
                            <li class="last"><a href="../../../index/../../../../doxygen_pages/html/group__shark__globals.html">Global functions</a></li>
                        </ul>
                    </li>
                </ul>

            </div>
        </div>
    </div>

      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
        <p class="logo"><a href="../../../index.html">
          <img class="logo" src="../../../_static/SharkLogo.png" alt="Logo"/></a></p>
	<div class="mt_ltocwrapper">
		<ul>
<li><a class="reference internal" href="#">Principal Component Analysis</a><ul>
<li><a class="reference internal" href="#background">Background</a></li>
<li><a class="reference internal" href="#pca-in-shark">PCA in Shark</a></li>
<li><a class="reference internal" href="#full-example-program">Full example program</a></li>
<li><a class="reference internal" href="#additional-features">Additional features</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

	</div>
<div>
  <a class="topless" href="../concepts/misc/serialization.html" title="previous chapter">
	  <img class="navicon" src="../../../_static/icon_backward.png" alt="prev"/> Serialization</a>
  <a class="topless" href="lda.html" title="next chapter">
	  <img class="navicon" src="../../../_static/icon_forward.png" alt="next"/> Linear Discriminant Analysis</a>
</div> 
<div id="searchbox" style="display: none">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" size="12" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
      <input class="mtsubmitbutton" type="submit" value="Find" />
    </form>
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
<p class="mtshowsource">
  <a href="../../../_sources/rest_sources/tutorials/algorithms/pca.rst.txt"
           rel="nofollow"><img class="sourceicon" src="../../../_static/icon_eject.png" alt="prev"/> Show page source</a>
</p>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="principal-component-analysis">
<h1>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this headline">¶</a></h1>
<div class="section" id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>Principal component analysis (PCA), also known as Karhunen-Loeve
transform, is arguably the most fundamental technique in
unUnsupervised learning. It is frequently used for (linear)
dimensionality reduction, (lossy) data compression, feature
extraction, and data visualization.  Let us consider a set of <em>l</em> data
points</p>
<div class="math notranslate">
\[S=\{x_1,\dots, x_l\} .\]</div>
<p>Let <em>n</em> real-valued attributes
(<em>n</em> features) represent a single data point. How can we reduce the
length of the description to <em>m &lt; n</em> variables such that as much
information as possible is preserved? How many dimensions are needed
to capture a certain percentage of the variability of the data? These
are typical questions that arise when we want to do dimensionality
reduction, feature selection, and compression (these three terms
usually refer to similar processes, emphasizing different aspects of
the same algorithmic procedure). Dimensionality reduction is closely
linked to visualization. When visualizing high-dimensional data, the
question arises of how to project the data to two or three dimensions
such that the visualization of these dimensions reflects as much of
the variability of the data as possible.  Principle component analysis
gives (under particular assumptions) answers to these questions.</p>
<p>The goal of PCA is to find an <em>m</em>-dimensional affine linear model of
the <em>n</em>-dimensional data points in <em>S</em> that represents the original
data as accurately as possible in a least-squares sense.  This model
does not only minimize the reconstruction error when mapping the data
back to the original space, it is also the affine linear model that
yields the representation that maximizes the overall variance when
encoding the points in <em>S</em> using only <em>m</em> dimensions. For details see
for instance <a class="reference internal" href="#dmln5" id="id1">[DMLN5]</a>.</p>
</div>
<div class="section" id="pca-in-shark">
<h2>PCA in Shark<a class="headerlink" href="#pca-in-shark" title="Permalink to this headline">¶</a></h2>
<p>The following includes are needed for this tutorial:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span> <span class="cpf">&lt;shark/Algorithms/Trainers/PCA.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;shark/Data/Pgm.h&gt;</span><span class="cp"></span>
</pre></div>
</div>
<div class="section" id="face-recognition-problem">
<h3>Face recognition problem<a class="headerlink" href="#face-recognition-problem" title="Permalink to this headline">¶</a></h3>
<p>As a classical example for PCA, let us consider computing eigenfaces
<a class="reference internal" href="#turkpentland1991" id="id2">[TurkPentland1991]</a> using the Cambridge Face Database
<a class="reference internal" href="#samariaharter1994" id="id3">[SamariaHarter1994]</a> as an example. It contains 92x122 images of
frontal faces, some examples are shown in Figure 5.2. We can represent
each face by a vector by inflating the image, that is, by
concatenating the image rows. Although the images are rather small, we
get a 10304-dimensional representation (i.e., <em>n = 10304</em>). That is,
in the original representation 10304 basis vectors, one for each
pixel, define the image space. Principal component analysis can help
us to significantly reduce the description length of the images.</p>
<p>The data can be downloaded from <a class="reference external" href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html">here</a>.
Some sample faces are shown in the following figure.</p>
<div class="figure">
<a class="reference internal image-reference" href="../../../_images/faces.png"><img alt="plot of sample faces" src="../../../_images/faces.png" style="width: 432.0px; height: 324.0px;" /></a>
</div>
</div>
<div class="section" id="reading-in-the-data">
<h3>Reading in the data<a class="headerlink" href="#reading-in-the-data" title="Permalink to this headline">¶</a></h3>
<p>First, let us read in the data.  There is a function for recursively
scanning a directory for images in pgm format and reading them in:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">facedirectory</span> <span class="o">=</span> <span class="s">&quot;Cambridge_FaceDB&quot;</span><span class="p">;</span> <span class="c1">//&lt; set this to the directory containing the face database</span>
<span class="n">UnlabeledData</span><span class="o">&lt;</span><span class="n">RealVector</span><span class="o">&gt;</span> <span class="n">images</span><span class="p">;</span>

        <span class="n">importPGMSet</span><span class="p">(</span><span class="n">facedirectory</span><span class="p">,</span> <span class="n">images</span><span class="p">);</span>

<span class="kt">unsigned</span> <span class="n">l</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">numberOfElements</span><span class="p">();</span>   <span class="c1">// number of samples</span>
<span class="kt">unsigned</span> <span class="n">x</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">];</span> <span class="c1">// width of images</span>
<span class="kt">unsigned</span> <span class="n">y</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">()[</span><span class="mi">0</span><span class="p">];</span> <span class="c1">// height of images</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">imagesInfo</span></code> struct contains sizes and names of the individual
images.</p>
</div>
<div class="section" id="models-and-learning-algorithm">
<h3>Models and learning algorithm<a class="headerlink" href="#models-and-learning-algorithm" title="Permalink to this headline">¶</a></h3>
<p>Doing a PCA is as simple as:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">PCA</span> <span class="nf">pca</span><span class="p">(</span><span class="n">images</span><span class="p">);</span>
</pre></div>
</div>
<p>Karhunen-Loeve transformations are affine linear models.
For encoding data to an <em>m</em>-dimensional subspace we use:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">unsigned</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">299</span><span class="p">;</span>
<span class="n">LinearModel</span><span class="o">&lt;&gt;</span> <span class="n">enc</span><span class="p">;</span>
<span class="n">pca</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">enc</span><span class="p">,</span> <span class="n">m</span><span class="p">);</span>
</pre></div>
</div>
<p>The last line encodes (i.e., represents in the PCA coordinate system)
the whole image database.</p>
<p>We can easily map from the <em>m</em> dimensional space back to the original
<em>n</em> dimensional space by the optimal linear reconstruction (the
decoder):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">LinearModel</span><span class="o">&lt;&gt;</span> <span class="n">dec</span><span class="p">;</span>
<span class="n">pca</span><span class="p">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">dec</span><span class="p">,</span> <span class="n">m</span><span class="p">);</span>
</pre></div>
</div>
<p>For instance, let us reconstruct the following first image using just the
first <em>m=300</em> components.</p>
<div class="figure">
<a class="reference internal image-reference" href="../../../_images/face0.png"><img alt="first face in database" src="../../../_images/face0.png" style="width: 92.0px; height: 112.0px;" /></a>
</div>
<p>Then we write</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">unsigned</span> <span class="n">sampleImage</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

<span class="n">boost</span><span class="o">::</span><span class="n">format</span> <span class="n">fmterRec</span><span class="p">(</span><span class="s">&quot;facesReconstruction%d-%d.pgm&quot;</span><span class="p">);</span>
<span class="n">exportPGM</span><span class="p">((</span><span class="n">fmterRec</span> <span class="o">%</span> <span class="n">sampleImage</span> <span class="o">%</span> <span class="n">m</span><span class="p">).</span><span class="n">str</span><span class="p">().</span><span class="n">c_str</span><span class="p">(),</span> <span class="n">dec</span><span class="p">(</span><span class="n">encodedImages</span><span class="p">.</span><span class="n">element</span><span class="p">(</span><span class="n">sampleImage</span><span class="p">)),</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
<p>and get the following image.</p>
<div class="figure">
<a class="reference internal image-reference" href="../../../_images/reconstruction0.png"><img alt="reconstruction of first face using 300 components" src="../../../_images/reconstruction0.png" style="width: 92.0px; height: 112.0px;" /></a>
</div>
</div>
<div class="section" id="further-evaluation-of-the-model">
<h3>Further evaluation of the model<a class="headerlink" href="#further-evaluation-of-the-model" title="Permalink to this headline">¶</a></h3>
<p>We can retrieve the eigenvalues and eigenvectors of the model by
calling <code class="docutils literal notranslate"><span class="pre">pca.eigenvalues()</span></code> and <code class="docutils literal notranslate"><span class="pre">pca.eigenvectors()</span></code>,
respectively.  The number of eigenvalues and eigenvectors returned by
these functions is min(<em>l</em>, <em>n</em>). The eigenvalue <em>i</em>  can also be
retrieved by <code class="docutils literal notranslate"><span class="pre">pca.eigenvalue(i)</span></code>.  Visualizing the mean face is done
by</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">exportPGM</span><span class="p">(</span><span class="s">&quot;facesMean.pgm&quot;</span><span class="p">,</span> <span class="n">pca</span><span class="p">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
<p>resulting in the following mean image.</p>
<div class="figure">
<a class="reference internal image-reference" href="../../../_images/facesMean.png"><img alt="reconstruction of first face using 200 components" src="../../../_images/facesMean.png" style="width: 92.0px; height: 112.0px;" /></a>
</div>
</div>
</div>
<div class="section" id="full-example-program">
<h2>Full example program<a class="headerlink" href="#full-example-program" title="Permalink to this headline">¶</a></h2>
<p>An extended eigenface example program is <a class="reference external" href="../../../../../../doxygen_pages/html/_p_c_a_tutorial_8cpp.html">PCATutorial.cpp</a>.
The face database can be downloaded from
<a class="reference external" href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html">here</a>.</p>
</div>
<div class="section" id="additional-features">
<h2>Additional features<a class="headerlink" href="#additional-features" title="Permalink to this headline">¶</a></h2>
<p>The Shark PCA automatically applies the “more attributes than data
points” trick, see <a class="reference internal" href="#dmln5" id="id5">[DMLN5]</a>. It easily allows for “whitening”, that
is, learning a transformation giving unit variance of the sample data
in the new coordinate system along each component.</p>
<p>As always, please look at the file documentation, the example
programs, and the unit test (<code class="docutils literal notranslate"><span class="pre">Test/Algorithms/Trainers</span></code>
subdirectory).</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<table class="docutils citation" frame="void" id="dmln5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[DMLN5]</td><td><em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id5">2</a>)</em> C. Igel.
Data Mining: Lecture Notes, chapter 5, 2011</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="samariaharter1994" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[SamariaHarter1994]</a></td><td>F. Samaria and A. Harter.
Parameterisation of a stochastic model for human face
identification. In IEEE Workshop on Applications of Computer
Vision, pages 138-142. IEEE Computer Society Press, 1994.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="turkpentland1991" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[TurkPentland1991]</a></td><td>M. Turk and A. Pentland. Eigenfaces for
recognition. Journal of Cognitive Neuroscience, 3(1):71-86, 1991.</td></tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

    <div class="footer">
        <div class="footerlogos">
            <a href="http://validator.w3.org/check/referer" title="Valid XHTML 1.0">
                <img class="footerlogos" src="../../../_static/xhtml_validation.png" alt="Valid XHTML 1.0" />
            </a>
            <a href="http://jigsaw.w3.org/css-validator/check/referer?profile=css3" title="Valid CSS3">
                <img class="footerlogos" src="../../../_static/css_validation.png" alt="Valid CSS3" />
            </a>
        </div>
            &copy; The Shark developer team.
           Created on 09/06/2018
           using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.7.2
    </div>
  </body>
</html>